{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', \n",
    "                                      origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
    "    extract=True)\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    #w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go .', 've .'],\n",
       " ['go .', 'vete .'],\n",
       " ['go .', 'vaya .'],\n",
       " ['go .', 'vayase .'],\n",
       " ['hi .', 'hola .']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val =create_dataset(path_to_file, 5)\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang, target=False):\n",
    "        self.lang = lang\n",
    "        self.target = target\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "          self.vocab.update(phrase.split(' '))\n",
    "    \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        \n",
    "        if self.target:\n",
    "            self.word2idx['<pad>'] = 0\n",
    "            self.word2idx['<START>'] = 1\n",
    "        else:\n",
    "            self.word2idx['<pad>'] = 0\n",
    "    \n",
    "        \n",
    "        for index, word in enumerate(self.vocab):\n",
    "          self.word2idx[word] = index + 1\n",
    "    \n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "    \n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex((en for en, sp in pairs),target=True)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # Spanish sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "    #print(input_tensor)\n",
    "    # English sentences\n",
    "    target_tensor = [[1]+[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    target_tensor_dec = [[1]+[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar, max_length_tar_dec = max_length(input_tensor), max_length(target_tensor),\\\n",
    "    max_length(target_tensor_dec)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,\n",
    "                                                                maxlen=max_length_inp,\n",
    "                                                                padding='post')\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
    "                                                                 maxlen=max_length_tar,\n",
    "                                                                 padding='post')\n",
    "    target_tensor_dec = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_dec,\n",
    "                                                                     maxlen=max_length_tar+1,\n",
    "                                                                     padding='post')\n",
    "    return input_tensor, target_tensor,target_tensor_dec, inp_lang, targ_lang, max_length_inp, max_length_tar,\\\n",
    "max_length_tar_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 3000\n",
    "input_tensor, target_tensor, target_tensor_dec, inp_lang, targ_lang, max_length_inp, max_length_targ, max_length_tar_dec = \\\n",
    "load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2400, 600, 600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n",
    "train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(data1, data2, data3, buffer_size, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data1,data2, data3)).shuffle(buffer_size)\n",
    "    dataset = dataset.map(lambda x, y, z: (x,y,z, tf.size(x),tf.size(y), tf.size(z)))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    batch_size = 128\n",
    "    buffer_size = len(input_tensor_train)//batch_size\n",
    "    unit = 1024\n",
    "    embedding_dim = 300\n",
    "    n_epoch = 2\n",
    "        \n",
    "    vocab_inp_size = len(inp_lang.word2idx)\n",
    "    vocab_tar_size = len(targ_lang.word2idx)\n",
    "    reverse_dict_input = inp_lang.idx2word\n",
    "    reverse_dict_target = targ_lang.idx2word\n",
    "    max_dec_inp_len = max_length_targ\n",
    "        \n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "    optimizer = tf.train.RMSPropOptimizer\n",
    "    dataset = dataset(input_tensor,target_tensor,target_tensor_dec, buffer_size, batch_size)\n",
    "    initializer = dataset.make_initializable_iterator()\n",
    "    input_data, input_data_dec, target_data_dec, inp_len,inp_dec_len,tar_dec_len = \\\n",
    "    initializer.get_next()\n",
    "        \n",
    "    #checkpoint Path\n",
    "    ckpt_dir = './ckpt_dir_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2token(idx, reverse_dict):\n",
    "    return reverse_dict[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_dict):\n",
    "    return ' '.join([idx2token(idx,reverse_dict) for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(object):\n",
    "    \n",
    "    def __init__(self, sess, config, mode):\n",
    "        self.mode = mode\n",
    "        self.learning_rate = config.learning_rate\n",
    "        self.batch_size = config.batch_size\n",
    "        self.n_epoch = config.n_epoch\n",
    "        \n",
    "        self.unit = config.unit\n",
    "        self.embedding_dim = config.embedding_dim\n",
    "        self.encoder_vocab_size = config.vocab_inp_size\n",
    "        self.decoder_vocab_size = config.vocab_tar_size\n",
    "        \n",
    "        self.cell = config.cell\n",
    "        self.optimizer = config.optimizer\n",
    "        \n",
    "        self.ckpt_path = config.ckpt_dir\n",
    "        #encoder input \n",
    "        self.input_data = config.input_data\n",
    "        self.inp_len = config.inp_len\n",
    "        # decoder input\n",
    "        self.input_data_dec = config.input_data_dec\n",
    "        self.inp_dec_len = config.inp_dec_len\n",
    "        self.tar_dec_len = config.tar_dec_len\n",
    "        self.max_length_targ = config.max_dec_inp_len\n",
    "        self.dec_reverse_dict = config.reverse_dict_target\n",
    "        # dataset initializer\n",
    "        self.initializer = config.initializer\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def encoder(self):\n",
    "        \n",
    "        with tf.variable_scope('encoder') as encoder:\n",
    "            self.encWemb = tf.get_variable(\n",
    "                'embedding',\n",
    "                initializer = tf.random_uniform([self.encoder_vocab_size, self.embedding_dim]),\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "        \n",
    "        self.enc_cell = self.cell(self.unit)\n",
    "        \n",
    "        enc_emb_input = tf.nn.embedding_lookup(self.encWemb, self.input_data, name='encoder_input')\n",
    "        \n",
    "        self.enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
    "            cell= self.enc_cell,\n",
    "            inputs = enc_emb_input,\n",
    "            sequense_length = sel.inp_len,\n",
    "            time_major = False,\n",
    "            dtype = tf.float32\n",
    "        )\n",
    "    \n",
    "    def decoder(self):\n",
    "        \n",
    "        with tf.variable_scope('decoder') as decoder:\n",
    "            self.decWemb = tf.get_variable(\n",
    "                'embedding',\n",
    "                initializer = tf.random_uniform([self.decoder_vocab_size, self.embedding_dim]),\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "        \n",
    "        self.dec_cell = self.cell(self.unit)\n",
    "        \n",
    "        dec_emb_input = tf.nn.embedding_lookup(self.decWemb, self.input_data_dec)\n",
    "        \n",
    "        output_layer = Dense(self.decoder_vocab_size, name='output_projection')\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            \n",
    "            train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_emb_input,\n",
    "                                                             sequence_length=self.inp_dec_len)\n",
    "            \n",
    "            train_decoder = tf.contrib.seq2seq.BasicDecoder(cell=self.dec_cell,\n",
    "                                                            helper=train_helper,\n",
    "                                                            initial_state=self.enc_last_state,\n",
    "                                                            output_layer=output_layer)\n",
    "            train_dec_output, train_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=train_decoder,\n",
    "                impute_finished=True,\n",
    "                maximum_iterations=self.max_length_targ)\n",
    "            \n",
    "            logits = tf.identity(train_dec_output.rnn_output, name='logits')\n",
    "            \n",
    "            masks = tf.sequence_mask(self.inp_dec_len, self.max_length_targ, dtype=float32,  name='masks')\n",
    "            \n",
    "            self.batch_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=logits,\n",
    "                targets=self.input_data_dec,\n",
    "                weights=masks,name='batch_loss'\n",
    "            )\n",
    "            \n",
    "            self.valid_predictions = tf.identity(train_dec_output.sample_id, name='valid_preds')\n",
    "        \n",
    "        elif self.mode == 'inference':\n",
    "            \n",
    "            batch_size = tf.shape(self.input_data)[0:1]\n",
    "            start_token = tf.ones(batch_size, dtype=tf.int32)\n",
    "            \n",
    "            infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                embedding=self.decWemb,\n",
    "                start_tokens=start_token,\n",
    "                end_token=0)\n",
    "            \n",
    "            infer_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = self.dec_cell,\n",
    "                helper=infer_helper, \n",
    "                initial_state=self.enc_last_state,\n",
    "                output_layer=output_layer)\n",
    "            \n",
    "            infer_dec_outputs, infer_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=infer.decoder,\n",
    "                impute_finished=True,\n",
    "                maximum_iterations=self.self.inp_dec_len)\n",
    "            \n",
    "            self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "\n",
    "    def add_training_op(self):\n",
    "        self.training_op = self.optimizer(self.learning_rate).minimize(self.batch_loss)\n",
    "    \n",
    "    \n",
    "    def saver(self, sess, var_list=None, save_path=None):\n",
    "        print('Saving model at {0}'.format(save_path))\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        saver = tf.train.Saver(var_list=var_list)\n",
    "        saver.save(sess, save_path, write_meta_graph=False)\n",
    "        \n",
    "    def restore(self, sess, var_list=None,save_path=None):\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        self.restorer = tf.train.Saver(var_list)\n",
    "        self.restorer.restore(sess, ckpt_path)\n",
    "        print('Restore Finished!')\n",
    "        \n",
    "    def summary(self):\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir = self.ckpt_path,\n",
    "            graph = tf.get_default_graph()\n",
    "        )\n",
    "        \n",
    "    def build(self):\n",
    "        self.encoder()\n",
    "        self.decoder()\n",
    "        \n",
    "    \n",
    "    def train(self,from_scratch=False, load_ckpt = None, save_path=None):\n",
    "        \n",
    "        if from_scratch is False and os.path.isfile(load_ckpt):\n",
    "            self.restore(sess, load_ckpt)\n",
    "        \n",
    "        \n",
    "        self.build()\n",
    "        self.add_training_op()\n",
    "        \n",
    "        \n",
    "        loss_history = []\n",
    "        for epoch in tqdm(range(self.n_epoch)):\n",
    "            all_preds = []\n",
    "            epoch_loss = 0\n",
    "            sess.run([self.initializer.initializer])\n",
    "            try:\n",
    "                while True:\n",
    "                    batch_pred, batch_loss, _ = sess.run([self.valid_predictions, self.batch_loss, self.training_op])\n",
    "                    epoch_loss += batch_loss\n",
    "                    all_preds.append(batch_pred)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "        \n",
    "            loss_history.append(epoch_loss)\n",
    "            \n",
    "            if epoch%2 == 0:\n",
    "                print('Epoch', epoch)\n",
    "                for input_batch, target_batch, batch_pred in zip(self.input_data,self.input_data_dec,batch_pred):\n",
    "                    for input_sent, target_sent, pred in zip(input_batch,target_batch, batch_pred):\n",
    "                        print('\\tinput sent', input_sent)\n",
    "                        print('\\tprediction', idx2sent(pred,self.dec_reverse_dict))\n",
    "                        print('\\tTarget:', target_sent)\n",
    "                print('\\tepoch loss: {epoch_loss:.2f}\\n')\n",
    "        if save_path:\n",
    "            self.saver(sess,save_path)\n",
    "        return loss_history\n",
    "    \n",
    "    def inference(self,load_ckpt):\n",
    "        self.restore(sess, save_path=load_ckpt)\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_tokens = []\n",
    "        batch_sent_lens = []\n",
    "        \n",
    "        batch_pred = sess.run([self.predictions])\n",
    "        for input_sent, target_sent, pred in zip(input_batch,target_batch, batch_pred):\n",
    "            print('\\tinput sent', input_sent)\n",
    "            print('\\tprediction', idx2sent(pred,self.dec_reverse_dict))\n",
    "            print('\\tTarget:', target_sent)\n",
    "                        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"IteratorGetNext:0\", shape=(?, 9), dtype=int32) must be from the same graph as Tensor(\"encoder/embedding:0\", shape=(1907, 300), dtype=float32_ref).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-41a8f01fa2e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_scratch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'epoch_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training model built!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-aa38df2b6a5d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, from_scratch, load_ckpt, save_path)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_training_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-aa38df2b6a5d>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-aa38df2b6a5d>\u001b[0m in \u001b[0;36mencoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0menc_emb_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencWemb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'encoder_input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         self.enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[1;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m       transform_fn=None)\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[1;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embedding_lookup\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m     \u001b[0mnp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Number of partitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;31m# Preserve the resource variable status to avoid accidental dense reads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6081\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6082\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6083\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6084\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6085\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5711\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5712\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5713\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5714\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5715\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5647\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5648\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5649\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"IteratorGetNext:0\", shape=(?, 9), dtype=int32) must be from the same graph as Tensor(\"encoder/embedding:0\", shape=(1907, 300), dtype=float32_ref)."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    config = Config()\n",
    "    model = seq2seq(sess, config, mode='training')\n",
    "    model.train(from_scratch=True, save_path=model.ckpt_path+'epoch_'+str(model.n_epoch))\n",
    "    print('Training model built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 792,    1,    0, ...,    0,    0,    0],\n",
       "        [ 406,    1,    0, ...,    0,    0,    0],\n",
       "        [1821,    3,    0, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1859,    5,  281, ...,    0,    0,    0],\n",
       "        [1123,  899, 1183, ...,    0,    0,    0],\n",
       "        [  11,    5, 1750, ...,    0,    0,    0]])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([config.initializer.initializer])\n",
    "sess.run([config.input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
